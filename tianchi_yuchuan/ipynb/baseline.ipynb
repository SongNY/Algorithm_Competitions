{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_feature(df, key, target, aggs):   \n",
    "    agg_dict = {}\n",
    "    for ag in aggs:\n",
    "        if isfunction(ag):\n",
    "            if (ag==pd.Series.kurt):\n",
    "                agg_dict[f'{target}_kurt'] = ag\n",
    "            elif (ag==q_25):\n",
    "                agg_dict[f'{target}_25'] = ag\n",
    "            elif (ag==q_75):\n",
    "                agg_dict[f'{target}_75'] = ag\n",
    "            else:\n",
    "                agg_dict[f'{target}_mode'] = ag\n",
    "        else:\n",
    "            agg_dict[f'{target}_{ag}'] = ag\n",
    "    print(agg_dict)\n",
    "    t = df.groupby(key)[target].agg(agg_dict).reset_index()\n",
    "    return t\n",
    "\n",
    "def q_25(x):\n",
    "    return x.quantile(q=0.25)\n",
    "def q_75(x):\n",
    "    return x.quantile(q=0.75)\n",
    "def modex(x):\n",
    "    return np.mean(pd.Series.mode(x))\n",
    "def corrxy(x):\n",
    "    return train['x'].corr(train['y'])\n",
    "\n",
    "def extract_feature(df, train):\n",
    "    t = group_feature(df, 'ship','x',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','x',['count'])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','y',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','v',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','d',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "\n",
    "    df['diffx']=df['x'].diff()\n",
    "    df['diffy']=df['y'].diff()\n",
    "    df['diffv']=df['v'].diff()\n",
    "    df['diffd']=df['d'].diff()\n",
    "    df['difflen']=(df['diffx']**2+df['diffy']**2)**0.5\n",
    "    t = group_feature(df, 'ship','diffx',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','diffy',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','diffv',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','diffd',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','difflen',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    \n",
    "    t=df[(df['v']==0)].groupby(['ship','x','y']).size().groupby('ship').idxmax().apply(pd.Series).iloc[:,1:]\n",
    "    t.rename(columns={1:'x_0', 2:'y_0'},inplace = True)\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    \n",
    "    train['x_0_x_max']=train['x_max']-train['x_0']\n",
    "    train['x_0_x_mean']=train['x_0']-train['x_mean']\n",
    "    train['x_0_x_min']=train['x_0']-train['x_min']\n",
    "    train['x_0_x_25']=train['x_0']-train['x_25']\n",
    "    train['x_0_x_median']=train['x_0']-train['x_median']\n",
    "    train['x_0_x_75']=train['x_0']-train['x_75']\n",
    "    train['x_0_x_mode']=train['x_0']-train['x_mode']\n",
    "\n",
    "    train['y_0_y_max']=train['y_max']-train['y_0']\n",
    "    train['y_0_y_mean']=train['y_0']-train['y_mean']\n",
    "    train['y_0_y_min']=train['y_0']-train['y_min']\n",
    "    train['y_0_y_25']=train['y_0']-train['y_25']\n",
    "    train['y_0_y_median']=train['y_0']-train['y_median']\n",
    "    train['y_0_y_75']=train['y_0']-train['y_75']\n",
    "    train['y_0_y_mode']=train['y_0']-train['y_mode']\n",
    "\n",
    "    train['x_max_x_min'] = train['x_max'] - train['x_min']\n",
    "    train['y_max_y_min'] = train['y_max'] - train['y_min']\n",
    "    \n",
    "    train['y_max_x_min'] = train['y_max'] - train['x_min']\n",
    "    train['y_max_x_max'] = train['y_max'] - train['x_max']\n",
    "    train['x_max_y_min'] = train['x_max'] - train['y_min']\n",
    "    train['x_min_y_min'] = train['x_min'] - train['y_min']\n",
    "    \n",
    "    train['slope'] = train['y_max_y_min'] / np.where(train['x_max_x_min']==0, 0.001, train['x_max_x_min'])\n",
    "    train['area'] = train['x_max_x_min'] * train['y_max_y_min']\n",
    "\n",
    "    t = df.groupby('ship')['time'].agg({'diff_time':lambda x:np.max(x)-np.min(x)}).reset_index()\n",
    "    t['diff_second'] = t['diff_time'].dt.seconds\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    \n",
    "    df['v_cos'] = df['v'] * np.cos(df['d'])\n",
    "    df['v_sin'] = df['v'] * np.sin(df['d'])\n",
    "    t = group_feature(df, 'ship','v_cos',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','v_sin',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')  \n",
    "    \n",
    "    df['k']=df['y']/df['x']\n",
    "    df['b']=df['y']-df['k'].mean()*df['x']\n",
    "    t = group_feature(df, 'ship','k',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df, 'ship','b',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    train = pd.merge(train, t, on='ship', how='left') \n",
    "    \n",
    "    df_night=df[(df['hour']<6) | (df['hour']>=18)]\n",
    "    t = group_feature(df_night, 'ship','x',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_night' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_night, 'ship','y',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_night' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_night, 'ship','v',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_night' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_night, 'ship','d',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_night' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    \n",
    "    df_day=df[(df['hour']>=6) & (df['hour']<18)]    \n",
    "    t = group_feature(df_day, 'ship','x',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_day' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_day, 'ship','y',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_day' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_day, 'ship','v',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_day' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_day, 'ship','d',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_day' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    \n",
    "    v_medain_list=np.repeat(df.groupby(['ship'])['v'].median().values,df.groupby(['ship'])['v'].count().values, axis=0)\n",
    "    df_vsmall=df[df['v']<v_medain_list]\n",
    "    t = group_feature(df_vsmall, 'ship','x',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vsamll' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vsmall, 'ship','y',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vsamll' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vsmall, 'ship','v',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vsamll' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vsmall, 'ship','d',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vsamll' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "\n",
    "    df_vlarge=df[df['v']>v_medain_list]\n",
    "    t = group_feature(df_vlarge, 'ship','x',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vlarge' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vlarge, 'ship','y',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vlarge' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vlarge, 'ship','v',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vlarge' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    t = group_feature(df_vlarge, 'ship','d',['max','min','median','mean','std','skew','sum',q_25,q_75,pd.Series.kurt,modex])\n",
    "    t.columns=['ship']+[x+'_vlarge' for x in t.columns[1:].tolist()]\n",
    "    train = pd.merge(train, t, on='ship', how='left')\n",
    "    return train\n",
    "\n",
    "def extract_dt(df):\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%m%d %H:%M:%S')\n",
    "    # df['month'] = df['time'].dt.month\n",
    "    # df['day'] = df['time'].dt.day\n",
    "    df['date'] = df['time'].dt.date\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    # df = df.drop_duplicates(['ship','month'])\n",
    "    df['weekday'] = df['time'].dt.weekday\n",
    "    return df\n",
    "\n",
    "def map_feature(train):\n",
    "    m,n=3,3\n",
    "    df=train\n",
    "    ymax,ymin,xmax,xmin=df['y'].max(),df['y'].min(),df['x'].max(),df['x'].min()\n",
    "    yy=(df['y']-ymin)/(ymax-ymin)\n",
    "    xx=(df['x']-xmin)/(xmax-xmin)\n",
    "    count,count10,vmean,dmean,vstd,dstd,xstd,ystd=[],[],[],[],[],[],[],[]\n",
    "    for i in range(m):\n",
    "        count.append([]),count10.append([]),vmean.append([]),dmean.append([]),dstd.append([]),vstd.append([]),xstd.append([]),ystd.append([])\n",
    "        for j in range(n):\n",
    "            if ((i==m)|(j==n)):\n",
    "                dfij=df[((i/m)<=yy) & (yy<=((i+1)/m)) & ((j/n)<=xx) & (xx<=((j+1)/n))]\n",
    "            else:\n",
    "                dfij=df[((i/m)<=yy) & (yy<((i+1)/m)) & ((j/n)<=xx) & (xx<((j+1)/n))]\n",
    "            cc=dfij.shape[0]\n",
    "            count[i].append(cc)\n",
    "            count10[i].append(cc!=0)\n",
    "            xstd[i].append(dfij['x'].std())\n",
    "            ystd[i].append(dfij['y'].std())\n",
    "            vmean[i].append(dfij['v'].mean())\n",
    "            vstd[i].append(dfij['v'].std())\n",
    "            dmean[i].append(dfij['d'].mean())\n",
    "            dstd[i].append(dfij['d'].std())\n",
    "    map_f=np.array([np.array(count),np.array(count10),np.array(vmean),np.array(dmean),\n",
    "                    np.array(vstd),np.array(dstd),np.array(xstd),np.array(ystd)]).reshape(m*n*8).T\n",
    "    return pd.DataFrame(map_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_hdf('/home/sunnyu/yuchuan/data/test.h5')\n",
    "test = extract_dt(test)\n",
    "test_label = test.drop_duplicates('ship')\n",
    "test_label = extract_feature(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m_f=test.groupby(['ship']).apply(map_feature).unstack()\n",
    "test_label=pd.merge(test_label, test_m_f, on='ship', how='left')\n",
    "test_label.columns=test_label.columns[:130].tolist()+[str(i) for i in range(36)]\n",
    "test_label.iloc[:,130:]=test_label.iloc[:,130:].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('/home/sunnyu/yuchuan/data/train2.h5')\n",
    "train = extract_dt(train)\n",
    "train_label = train.drop_duplicates('ship')\n",
    "type_map = dict(zip(train_label['type'].unique(), np.arange(3)))\n",
    "type_map_rev = {v:k for k,v in type_map.items()}\n",
    "train_label['type'] = train_label['type'].map(type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_max': 'max', 'x_min': 'min', 'x_median': 'median', 'x_mean': 'mean', 'x_std': 'std', 'x_skew': 'skew', 'x_sum': 'sum', 'x_25': <function q_25 at 0x7f232e2ff440>, 'x_75': <function q_75 at 0x7f232e2ff710>, 'x_kurt': <function Series.kurt at 0x7f235107d050>, 'x_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'x_count': 'count'}\n",
      "{'y_max': 'max', 'y_min': 'min', 'y_median': 'median', 'y_mean': 'mean', 'y_std': 'std', 'y_skew': 'skew', 'y_sum': 'sum', 'y_25': <function q_25 at 0x7f232e2ff440>, 'y_75': <function q_75 at 0x7f232e2ff710>, 'y_kurt': <function Series.kurt at 0x7f235107d050>, 'y_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_max': 'max', 'v_min': 'min', 'v_median': 'median', 'v_mean': 'mean', 'v_std': 'std', 'v_skew': 'skew', 'v_sum': 'sum', 'v_25': <function q_25 at 0x7f232e2ff440>, 'v_75': <function q_75 at 0x7f232e2ff710>, 'v_kurt': <function Series.kurt at 0x7f235107d050>, 'v_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'d_max': 'max', 'd_min': 'min', 'd_median': 'median', 'd_mean': 'mean', 'd_std': 'std', 'd_skew': 'skew', 'd_sum': 'sum', 'd_25': <function q_25 at 0x7f232e2ff440>, 'd_75': <function q_75 at 0x7f232e2ff710>, 'd_kurt': <function Series.kurt at 0x7f235107d050>, 'd_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'diffx_max': 'max', 'diffx_min': 'min', 'diffx_median': 'median', 'diffx_mean': 'mean', 'diffx_std': 'std', 'diffx_skew': 'skew', 'diffx_sum': 'sum', 'diffx_25': <function q_25 at 0x7f232e2ff440>, 'diffx_75': <function q_75 at 0x7f232e2ff710>, 'diffx_kurt': <function Series.kurt at 0x7f235107d050>, 'diffx_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'diffy_max': 'max', 'diffy_min': 'min', 'diffy_median': 'median', 'diffy_mean': 'mean', 'diffy_std': 'std', 'diffy_skew': 'skew', 'diffy_sum': 'sum', 'diffy_25': <function q_25 at 0x7f232e2ff440>, 'diffy_75': <function q_75 at 0x7f232e2ff710>, 'diffy_kurt': <function Series.kurt at 0x7f235107d050>, 'diffy_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'diffv_max': 'max', 'diffv_min': 'min', 'diffv_median': 'median', 'diffv_mean': 'mean', 'diffv_std': 'std', 'diffv_skew': 'skew', 'diffv_sum': 'sum', 'diffv_25': <function q_25 at 0x7f232e2ff440>, 'diffv_75': <function q_75 at 0x7f232e2ff710>, 'diffv_kurt': <function Series.kurt at 0x7f235107d050>, 'diffv_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'diffd_max': 'max', 'diffd_min': 'min', 'diffd_median': 'median', 'diffd_mean': 'mean', 'diffd_std': 'std', 'diffd_skew': 'skew', 'diffd_sum': 'sum', 'diffd_25': <function q_25 at 0x7f232e2ff440>, 'diffd_75': <function q_75 at 0x7f232e2ff710>, 'diffd_kurt': <function Series.kurt at 0x7f235107d050>, 'diffd_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'difflen_max': 'max', 'difflen_min': 'min', 'difflen_median': 'median', 'difflen_mean': 'mean', 'difflen_std': 'std', 'difflen_skew': 'skew', 'difflen_sum': 'sum', 'difflen_25': <function q_25 at 0x7f232e2ff440>, 'difflen_75': <function q_75 at 0x7f232e2ff710>, 'difflen_kurt': <function Series.kurt at 0x7f235107d050>, 'difflen_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_cos_max': 'max', 'v_cos_min': 'min', 'v_cos_median': 'median', 'v_cos_mean': 'mean', 'v_cos_std': 'std', 'v_cos_skew': 'skew', 'v_cos_sum': 'sum', 'v_cos_25': <function q_25 at 0x7f232e2ff440>, 'v_cos_75': <function q_75 at 0x7f232e2ff710>, 'v_cos_kurt': <function Series.kurt at 0x7f235107d050>, 'v_cos_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_sin_max': 'max', 'v_sin_min': 'min', 'v_sin_median': 'median', 'v_sin_mean': 'mean', 'v_sin_std': 'std', 'v_sin_skew': 'skew', 'v_sin_sum': 'sum', 'v_sin_25': <function q_25 at 0x7f232e2ff440>, 'v_sin_75': <function q_75 at 0x7f232e2ff710>, 'v_sin_kurt': <function Series.kurt at 0x7f235107d050>, 'v_sin_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'k_max': 'max', 'k_min': 'min', 'k_median': 'median', 'k_mean': 'mean', 'k_std': 'std', 'k_skew': 'skew', 'k_sum': 'sum', 'k_25': <function q_25 at 0x7f232e2ff440>, 'k_75': <function q_75 at 0x7f232e2ff710>, 'k_kurt': <function Series.kurt at 0x7f235107d050>, 'k_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'b_max': 'max', 'b_min': 'min', 'b_median': 'median', 'b_mean': 'mean', 'b_std': 'std', 'b_skew': 'skew', 'b_sum': 'sum', 'b_25': <function q_25 at 0x7f232e2ff440>, 'b_75': <function q_75 at 0x7f232e2ff710>, 'b_kurt': <function Series.kurt at 0x7f235107d050>, 'b_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'x_max': 'max', 'x_min': 'min', 'x_median': 'median', 'x_mean': 'mean', 'x_std': 'std', 'x_skew': 'skew', 'x_sum': 'sum', 'x_25': <function q_25 at 0x7f232e2ff440>, 'x_75': <function q_75 at 0x7f232e2ff710>, 'x_kurt': <function Series.kurt at 0x7f235107d050>, 'x_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'y_max': 'max', 'y_min': 'min', 'y_median': 'median', 'y_mean': 'mean', 'y_std': 'std', 'y_skew': 'skew', 'y_sum': 'sum', 'y_25': <function q_25 at 0x7f232e2ff440>, 'y_75': <function q_75 at 0x7f232e2ff710>, 'y_kurt': <function Series.kurt at 0x7f235107d050>, 'y_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_max': 'max', 'v_min': 'min', 'v_median': 'median', 'v_mean': 'mean', 'v_std': 'std', 'v_skew': 'skew', 'v_sum': 'sum', 'v_25': <function q_25 at 0x7f232e2ff440>, 'v_75': <function q_75 at 0x7f232e2ff710>, 'v_kurt': <function Series.kurt at 0x7f235107d050>, 'v_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'d_max': 'max', 'd_min': 'min', 'd_median': 'median', 'd_mean': 'mean', 'd_std': 'std', 'd_skew': 'skew', 'd_sum': 'sum', 'd_25': <function q_25 at 0x7f232e2ff440>, 'd_75': <function q_75 at 0x7f232e2ff710>, 'd_kurt': <function Series.kurt at 0x7f235107d050>, 'd_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'x_max': 'max', 'x_min': 'min', 'x_median': 'median', 'x_mean': 'mean', 'x_std': 'std', 'x_skew': 'skew', 'x_sum': 'sum', 'x_25': <function q_25 at 0x7f232e2ff440>, 'x_75': <function q_75 at 0x7f232e2ff710>, 'x_kurt': <function Series.kurt at 0x7f235107d050>, 'x_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'y_max': 'max', 'y_min': 'min', 'y_median': 'median', 'y_mean': 'mean', 'y_std': 'std', 'y_skew': 'skew', 'y_sum': 'sum', 'y_25': <function q_25 at 0x7f232e2ff440>, 'y_75': <function q_75 at 0x7f232e2ff710>, 'y_kurt': <function Series.kurt at 0x7f235107d050>, 'y_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_max': 'max', 'v_min': 'min', 'v_median': 'median', 'v_mean': 'mean', 'v_std': 'std', 'v_skew': 'skew', 'v_sum': 'sum', 'v_25': <function q_25 at 0x7f232e2ff440>, 'v_75': <function q_75 at 0x7f232e2ff710>, 'v_kurt': <function Series.kurt at 0x7f235107d050>, 'v_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'d_max': 'max', 'd_min': 'min', 'd_median': 'median', 'd_mean': 'mean', 'd_std': 'std', 'd_skew': 'skew', 'd_sum': 'sum', 'd_25': <function q_25 at 0x7f232e2ff440>, 'd_75': <function q_75 at 0x7f232e2ff710>, 'd_kurt': <function Series.kurt at 0x7f235107d050>, 'd_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'x_max': 'max', 'x_min': 'min', 'x_median': 'median', 'x_mean': 'mean', 'x_std': 'std', 'x_skew': 'skew', 'x_sum': 'sum', 'x_25': <function q_25 at 0x7f232e2ff440>, 'x_75': <function q_75 at 0x7f232e2ff710>, 'x_kurt': <function Series.kurt at 0x7f235107d050>, 'x_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'y_max': 'max', 'y_min': 'min', 'y_median': 'median', 'y_mean': 'mean', 'y_std': 'std', 'y_skew': 'skew', 'y_sum': 'sum', 'y_25': <function q_25 at 0x7f232e2ff440>, 'y_75': <function q_75 at 0x7f232e2ff710>, 'y_kurt': <function Series.kurt at 0x7f235107d050>, 'y_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_max': 'max', 'v_min': 'min', 'v_median': 'median', 'v_mean': 'mean', 'v_std': 'std', 'v_skew': 'skew', 'v_sum': 'sum', 'v_25': <function q_25 at 0x7f232e2ff440>, 'v_75': <function q_75 at 0x7f232e2ff710>, 'v_kurt': <function Series.kurt at 0x7f235107d050>, 'v_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'d_max': 'max', 'd_min': 'min', 'd_median': 'median', 'd_mean': 'mean', 'd_std': 'std', 'd_skew': 'skew', 'd_sum': 'sum', 'd_25': <function q_25 at 0x7f232e2ff440>, 'd_75': <function q_75 at 0x7f232e2ff710>, 'd_kurt': <function Series.kurt at 0x7f235107d050>, 'd_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'x_max': 'max', 'x_min': 'min', 'x_median': 'median', 'x_mean': 'mean', 'x_std': 'std', 'x_skew': 'skew', 'x_sum': 'sum', 'x_25': <function q_25 at 0x7f232e2ff440>, 'x_75': <function q_75 at 0x7f232e2ff710>, 'x_kurt': <function Series.kurt at 0x7f235107d050>, 'x_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'y_max': 'max', 'y_min': 'min', 'y_median': 'median', 'y_mean': 'mean', 'y_std': 'std', 'y_skew': 'skew', 'y_sum': 'sum', 'y_25': <function q_25 at 0x7f232e2ff440>, 'y_75': <function q_75 at 0x7f232e2ff710>, 'y_kurt': <function Series.kurt at 0x7f235107d050>, 'y_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'v_max': 'max', 'v_min': 'min', 'v_median': 'median', 'v_mean': 'mean', 'v_std': 'std', 'v_skew': 'skew', 'v_sum': 'sum', 'v_25': <function q_25 at 0x7f232e2ff440>, 'v_75': <function q_75 at 0x7f232e2ff710>, 'v_kurt': <function Series.kurt at 0x7f235107d050>, 'v_mode': <function modex at 0x7f2329fddcb0>}\n",
      "{'d_max': 'max', 'd_min': 'min', 'd_median': 'median', 'd_mean': 'mean', 'd_std': 'std', 'd_skew': 'skew', 'd_sum': 'sum', 'd_25': <function q_25 at 0x7f232e2ff440>, 'd_75': <function q_75 at 0x7f232e2ff710>, 'd_kurt': <function Series.kurt at 0x7f235107d050>, 'd_mode': <function modex at 0x7f2329fddcb0>}\n"
     ]
    }
   ],
   "source": [
    "train_label = extract_feature(train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m_f=train.groupby(['ship']).apply(map_feature).unstack()\n",
    "train_m_f.columns=[str(i) for i in range(72)]\n",
    "train_label=pd.merge(train_label, train_m_f, on='ship', how='left')\n",
    "# train_label.iloc[:,125:]=train_label.iloc[:,125:].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_co=train.groupby('ship').apply(lambda x :x['x'].corr(x['y'])).reset_index()\n",
    "train_co.columns=['ship','corr']\n",
    "train_label=pd.merge(train_label, train_co, on='ship', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_co=test.groupby('ship').apply(lambda x :x['x'].corr(x['y'])).reset_index()\n",
    "test_co.columns=['ship','corr']\n",
    "test_label=pd.merge(test_label, test_co, on='ship', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cnn_feature=pd.read_csv('test_cnn_feature.csv')\n",
    "train_cnn_feature=pd.read_csv('train_cnn_feature.csv')\n",
    "train_cnn_feature.columns=['ship']+[str(i)+'_cnn' for i in range(16)]\n",
    "test_cnn_feature.columns=['ship']+[str(i)+'_cnn' for i in range(16)]\n",
    "train_label=pd.merge(train_label, train_cnn_feature, on='ship', how='left')\n",
    "test_label=pd.merge(test_label, test_cnn_feature, on='ship', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label.drop(columns=train_label.columns[-37:-1],inplace=True)\n",
    "# test_label.drop(columns=test_label.columns[-32:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, array(['x', 'y', 'v', 'd', 'hour', 'weekday', 'x_max', 'x_min',\n",
       "        'x_median', 'x_mean', 'x_std', 'x_skew', 'x_sum', 'x_25', 'x_75',\n",
       "        'x_kurt', 'x_mode', 'x_count', 'y_max', 'y_min', 'y_median',\n",
       "        'y_mean', 'y_std', 'y_skew', 'y_sum', 'y_25', 'y_75', 'y_kurt',\n",
       "        'y_mode', 'v_max', 'v_min', 'v_median', 'v_mean', 'v_std',\n",
       "        'v_skew', 'v_sum', 'v_25', 'v_75', 'v_kurt', 'v_mode', 'd_max',\n",
       "        'd_min', 'd_median', 'd_mean', 'd_std', 'd_skew', 'd_sum', 'd_25',\n",
       "        'd_75', 'd_kurt', 'd_mode', 'diffx_max', 'diffx_min',\n",
       "        'diffx_median', 'diffx_mean', 'diffx_std', 'diffx_skew',\n",
       "        'diffx_sum', 'diffx_25', 'diffx_75', 'diffx_kurt', 'diffx_mode',\n",
       "        'diffy_max', 'diffy_min', 'diffy_median', 'diffy_mean',\n",
       "        'diffy_std', 'diffy_skew', 'diffy_sum', 'diffy_25', 'diffy_75',\n",
       "        'diffy_kurt', 'diffy_mode', 'diffv_max', 'diffv_min',\n",
       "        'diffv_median', 'diffv_mean', 'diffv_std', 'diffv_skew',\n",
       "        'diffv_sum', 'diffv_25', 'diffv_75', 'diffv_kurt', 'diffv_mode',\n",
       "        'diffd_max', 'diffd_min', 'diffd_median', 'diffd_mean',\n",
       "        'diffd_std', 'diffd_skew', 'diffd_sum', 'diffd_25', 'diffd_75',\n",
       "        'diffd_kurt', 'diffd_mode', 'difflen_max', 'difflen_min',\n",
       "        'difflen_median', 'difflen_mean', 'difflen_std', 'difflen_skew',\n",
       "        'difflen_sum', 'difflen_25', 'difflen_75', 'difflen_kurt',\n",
       "        'difflen_mode', 'x_0', 'y_0', 'x_0_x_max', 'x_0_x_mean',\n",
       "        'x_0_x_min', 'x_0_x_25', 'x_0_x_median', 'x_0_x_75', 'x_0_x_mode',\n",
       "        'y_0_y_max', 'y_0_y_mean', 'y_0_y_min', 'y_0_y_25', 'y_0_y_median',\n",
       "        'y_0_y_75', 'y_0_y_mode', 'x_max_x_min', 'y_max_y_min',\n",
       "        'y_max_x_min', 'y_max_x_max', 'x_max_y_min', 'x_min_y_min',\n",
       "        'slope', 'area', 'diff_second', 'v_cos_max', 'v_cos_min',\n",
       "        'v_cos_median', 'v_cos_mean', 'v_cos_std', 'v_cos_skew',\n",
       "        'v_cos_sum', 'v_cos_25', 'v_cos_75', 'v_cos_kurt', 'v_cos_mode',\n",
       "        'v_sin_max', 'v_sin_min', 'v_sin_median', 'v_sin_mean',\n",
       "        'v_sin_std', 'v_sin_skew', 'v_sin_sum', 'v_sin_25', 'v_sin_75',\n",
       "        'v_sin_kurt', 'v_sin_mode', 'k_max', 'k_min', 'k_median', 'k_mean',\n",
       "        'k_std', 'k_skew', 'k_sum', 'k_25', 'k_75', 'k_kurt', 'k_mode',\n",
       "        'b_max', 'b_min', 'b_median', 'b_mean', 'b_std', 'b_skew', 'b_sum',\n",
       "        'b_25', 'b_75', 'b_kurt', 'b_mode', 'x_max_night', 'x_min_night',\n",
       "        'x_median_night', 'x_mean_night', 'x_std_night', 'x_skew_night',\n",
       "        'x_sum_night', 'x_25_night', 'x_75_night', 'x_kurt_night',\n",
       "        'x_mode_night', 'y_max_night', 'y_min_night', 'y_median_night',\n",
       "        'y_mean_night', 'y_std_night', 'y_skew_night', 'y_sum_night',\n",
       "        'y_25_night', 'y_75_night', 'y_kurt_night', 'y_mode_night',\n",
       "        'v_max_night', 'v_min_night', 'v_median_night', 'v_mean_night',\n",
       "        'v_std_night', 'v_skew_night', 'v_sum_night', 'v_25_night',\n",
       "        'v_75_night', 'v_kurt_night', 'v_mode_night', 'd_max_night',\n",
       "        'd_min_night', 'd_median_night', 'd_mean_night', 'd_std_night',\n",
       "        'd_skew_night', 'd_sum_night', 'd_25_night', 'd_75_night',\n",
       "        'd_kurt_night', 'd_mode_night', 'x_max_day', 'x_min_day',\n",
       "        'x_median_day', 'x_mean_day', 'x_std_day', 'x_skew_day',\n",
       "        'x_sum_day', 'x_25_day', 'x_75_day', 'x_kurt_day', 'x_mode_day',\n",
       "        'y_max_day', 'y_min_day', 'y_median_day', 'y_mean_day',\n",
       "        'y_std_day', 'y_skew_day', 'y_sum_day', 'y_25_day', 'y_75_day',\n",
       "        'y_kurt_day', 'y_mode_day', 'v_max_day', 'v_min_day',\n",
       "        'v_median_day', 'v_mean_day', 'v_std_day', 'v_skew_day',\n",
       "        'v_sum_day', 'v_25_day', 'v_75_day', 'v_kurt_day', 'v_mode_day',\n",
       "        'd_max_day', 'd_min_day', 'd_median_day', 'd_mean_day',\n",
       "        'd_std_day', 'd_skew_day', 'd_sum_day', 'd_25_day', 'd_75_day',\n",
       "        'd_kurt_day', 'd_mode_day', '0', '1', '2', '3', '4', '5', '6', '7',\n",
       "        '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
       "        '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n",
       "        '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40',\n",
       "        '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51',\n",
       "        '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62',\n",
       "        '63', '64', '65', '66', '67', '68', '69', '70', '71', 'corr'],\n",
       "       dtype='<U14'))"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [x for x in train_label.columns if x not in ['ship','type','time','diff_time','date',\n",
    "\n",
    "#                                                       'x','y','v','d',                                                    \n",
    "#                                                          'x_0_x_25','x_0_x_median','x_0_x_75','x_0_x_mean','x_0_x_max','x_0_x_min','x_0_x_mode',\n",
    "#                                                          'y_0_y_25','y_0_y_median','y_0_y_75','y_0_y_mean','y_0_y_max','y_0_y_min','y_0_y_mode',\n",
    "#                                                          'y_max_x_min','y_max_x_max','x_max_y_min','x_min_y_min'\n",
    "                                                       ]+train_label.columns[268:-73].values.tolist()]\n",
    "target = 'type'\n",
    "len(features),np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "     'num_leaves':64,\n",
    "#        'max_depth':6, \n",
    "    'lambda_l1':0.1,\n",
    "#     'lambda_l2':0.2,\n",
    "    'n_estimators': 200,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "#     'early_stopping_rounds': 200,\n",
    "    'num_threads':20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's multi_logloss: 0.0102085\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.285088\tvalid_1's f1-score: 0.880102\n",
      "[200]\ttraining's multi_logloss: 0.00157557\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.327029\tvalid_1's f1-score: 0.882175\n",
      "0 val f1 0.8821751537838004\n",
      "[100]\ttraining's multi_logloss: 0.0101008\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.253507\tvalid_1's f1-score: 0.886134\n",
      "[200]\ttraining's multi_logloss: 0.00156062\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.279855\tvalid_1's f1-score: 0.894683\n",
      "1 val f1 0.8946830628229248\n",
      "[100]\ttraining's multi_logloss: 0.00993858\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.278343\tvalid_1's f1-score: 0.880296\n",
      "[200]\ttraining's multi_logloss: 0.00155869\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.316421\tvalid_1's f1-score: 0.88588\n",
      "2 val f1 0.8858802277319885\n",
      "[100]\ttraining's multi_logloss: 0.0100595\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.286028\tvalid_1's f1-score: 0.883848\n",
      "[200]\ttraining's multi_logloss: 0.00155435\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.324308\tvalid_1's f1-score: 0.887626\n",
      "3 val f1 0.8876261845001019\n",
      "[100]\ttraining's multi_logloss: 0.0103814\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.225641\tvalid_1's f1-score: 0.896805\n",
      "[200]\ttraining's multi_logloss: 0.00161622\ttraining's f1-score: 1\tvalid_1's multi_logloss: 0.255938\tvalid_1's f1-score: 0.901792\n",
      "4 val f1 0.9017919847697887\n"
     ]
    }
   ],
   "source": [
    "def mac_f1(predss,mm):\n",
    "    labell=mm.get_label().values\n",
    "    predss=np.argmax(predss.reshape(3,len(labell)), axis=0)\n",
    "    f11=metrics.f1_score(labell,predss,average='macro')\n",
    "    return 'f1-score',f11,True\n",
    "\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=513)\n",
    "\n",
    "X = train_label[features].copy()\n",
    "y = train_label[target]\n",
    "models = []\n",
    "#a=[350,300,400,450,350]\n",
    "i=0\n",
    "\n",
    "# pred = np.zeros((len(test_label),3))\n",
    "oof = np.zeros((len(X), 3))\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n",
    "\n",
    "    train_set = lgb.Dataset(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    val_set = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n",
    "#    params['n_estimators']=a[i]\n",
    "    i+=1\n",
    "    model = lgb.train(params, train_set, valid_sets=[train_set,val_set],feval=mac_f1, verbose_eval=100)\n",
    "    models.append(model)\n",
    "    val_pred = model.predict(X.iloc[val_idx])\n",
    "    oof[val_idx] = val_pred\n",
    "    val_y = y.iloc[val_idx]\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    print(index, 'val f1', metrics.f1_score(val_y, val_pred, average='macro'))\n",
    "    # 0.8695539641133697\n",
    "    # 0.8866211724839532\n",
    "\n",
    "#     test_pred = model.predict(test_label[features])\n",
    "#     pred += test_pred/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    models[i].save_model('/home/sunnyu/yuchuan/sub/fusaisub/model/lgb'+str(i)+'.txt')\n",
    "pd.DataFrame(features).to_csv('/home/sunnyu/yuchuan/sub/fusaisub/model/features.csv',index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof f1 0.8904708820452548\n"
     ]
    }
   ],
   "source": [
    "oof = np.argmax(oof, axis=1)\n",
    "print('oof f1', metrics.f1_score(oof, y, average='macro'))\n",
    "# 0.8701544575329372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for index, model in enumerate(models):\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = model.feature_name()\n",
    "    df['score'] = model.feature_importance()\n",
    "    df['fold'] = index\n",
    "    ret.append(df)\n",
    "    \n",
    "df = pd.concat(ret)\n",
    "df = df.groupby('name', as_index=False)['score'].mean()\n",
    "df = df.sort_values(['score'],  ascending=False)\n",
    "del_f_sp=df[df['score']>=400]['name'].tolist()\n",
    "len(del_f_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for index, model in enumerate(models):\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = model.feature_name()\n",
    "    df['score'] = model.feature_importance('gain')\n",
    "    df['fold'] = index\n",
    "    ret.append(df)\n",
    "    \n",
    "df = pd.concat(ret)\n",
    "df = df.groupby('name', as_index=False)['score'].mean()\n",
    "df = df.sort_values(['score'],  ascending=False)\n",
    "del_f_ga=df[df['score']>=200]['name'].tolist()\n",
    "len(del_f_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62,\n",
       " array({'v_std', 'v_std_x', '25', 'v_75_y', 'x_min_y', 'v_mean_y', 'b_75', 'y_max_x_max', 'diffx_25', 'x_max_x', 'v_median_y', 'y_median_x', '18', 'v_median_x', 'y', 'y_mode_y', 'x_max_y_min', 'k_75', '24', 'y_mode_x', 'x_mode_x', 'x_mode_y', 'y_25_y', 'y_min_x', 'diffy_25', 'x_75_x', 'x_min_x', 'v_median', 'b_min', 'k_25', 'x_mode', 'b_median', 'b_mode', 'k_mode', 'x', 'k_median', 'slope', 'd_std', 'diff_second', 'y_max_x_min', 'b_25', 'diffd_std', 'x_min_y_min', 'y_min_y', 'b_max', 'y_std', 'b_sum', 'v_75_x', 'diffd_25', 'v_kurt_x', 'diffd_kurt', 'v_75', 'k_min', 'x_max', 'v_kurt_y', 'y_0', 'x_min', 'k_max', 'y_min', 'x_max_y', 'x_0', 'diffd_75'},\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=set(del_f_ga)\n",
    "len(features),np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for index, model in enumerate(models):\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = model.feature_name()\n",
    "    df['score'] = model.feature_importance('gain')\n",
    "    df['fold'] = index\n",
    "    ret.append(df)\n",
    "    \n",
    "df = pd.concat(ret)\n",
    "df = df.groupby('name', as_index=False)['score'].mean()\n",
    "df = df.sort_values(['score'],  ascending=False)\n",
    "del_f=df[df['score']<=150]['name'].tolist()\n",
    "len(del_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, array(['x', 'y', 'x_min', 'x_mode', 'y_min', 'y_median', 'y_25', 'y_mode',\n",
       "        'v_std', 'v_75', 'v_kurt', 'diffx_25', 'diffy_25', 'diffd_std',\n",
       "        'diffd_25', 'diffd_75', 'diffd_kurt', 'difflen_median',\n",
       "        'difflen_75', 'x_0', 'y_0', 'y_max_x_min', 'y_max_x_max',\n",
       "        'x_max_y_min', 'x_min_y_min', 'diff_second', 'k_max', 'k_min',\n",
       "        'k_median', 'k_sum', 'k_25', 'k_75', 'k_mode', 'b_max', 'b_min',\n",
       "        'b_median', 'b_25', 'b_75', 'b_mode', 'x_max_night', 'x_75_night',\n",
       "        'x_mode_night', 'y_min_night', 'y_mode_night', 'v_median_night',\n",
       "        'v_75_night', 'v_kurt_night', 'x_max_day', 'x_min_day', 'x_75_day',\n",
       "        'x_mode_day', 'y_min_day', 'y_mode_day', 'v_median_day',\n",
       "        'v_std_day', 'v_75_day', 'v_kurt_day', 'd_std_day', '24', '36',\n",
       "        '44', 'corr'], dtype='<U14'))"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [x for x in train_label.columns if x not in ['ship','type','time','diff_time','date',\n",
    "#                                                        'x','y','v','d',\n",
    "#                                                          'x_0_x_25','x_0_x_median','x_0_x_75','x_0_x_mean','x_0_x_max','x_0_x_min','x_0_x_mode',\n",
    "#                                                          'y_0_y_25','y_0_y_median','y_0_y_75','y_0_y_mean','y_0_y_max','y_0_y_min','y_0_y_mode',\n",
    "#                                                          'y_max_x_min','y_max_x_max','x_max_y_min','x_min_y_min'\n",
    "                                                       ]+del_f+train_label.columns[268:-73].values.tolist()]\n",
    "target = 'type'\n",
    "len(features),np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pd.read_csv('~/yuchuan/sub/best_model/feature.csv',header=None)[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves':40,\n",
    "#        'max_depth':6,\n",
    "    'n_estimators':2500,\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'multiclassova',\n",
    "    'num_class': 3,\n",
    "    'num_threads':20,\n",
    "    #     'early_stopping_rounds': 200,\n",
    "#     'lambda_l1':0.1,\n",
    "#   'lambda_l2':0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val f1 0.9182937946946083\n",
      "1 val f1 0.9173847913333896\n",
      "2 val f1 0.9207770103300387\n",
      "3 val f1 0.9100071418172314\n",
      "4 val f1 0.927211641561838\n",
      "5 val f1 0.9251732473811441\n"
     ]
    }
   ],
   "source": [
    "fold = StratifiedKFold(n_splits=6, shuffle=True, random_state=513)\n",
    "\n",
    "X = train_label[features].copy()\n",
    "y = train_label[target]\n",
    "models = []\n",
    "a40=[900,1450,1370,630,750,610]\n",
    "i=0\n",
    "\n",
    "oof = np.zeros((len(X), 3))\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n",
    "\n",
    "    X_train = pd.concat([X.iloc[train_idx],X.iloc[train_idx][train_label.iloc[train_idx]['type']==1]])\n",
    "    y_train = pd.concat([y.iloc[train_idx],y.iloc[train_idx][train_label.iloc[train_idx]['type']==1]])   \n",
    "    train_set = lgb.Dataset(X_train, y_train)\n",
    "    val_set = lgb.Dataset(X.iloc[val_idx], y.iloc[val_idx])\n",
    "    params['n_estimators']=a40[i]\n",
    "    i+=1\n",
    "    model = lgb.train(params, train_set, valid_sets=val_set,feval=mac_f1, verbose_eval=1500)\n",
    "    models.append(model)\n",
    "    val_pred = model.predict(X.iloc[val_idx])\n",
    "    oof[val_idx] = val_pred\n",
    "    val_y = y.iloc[val_idx]\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    print(index, 'val f1', metrics.f1_score(val_y, val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    models[i].save_model('/home/sunnyu/yuchuan/sub/fusaisub/model/lgb'+str(i)+'.txt')\n",
    "pd.DataFrame(features).to_csv('/home/sunnyu/yuchuan/sub/fusaisub/model/features.csv',index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'拖网': 0, '刺网': 1, '围网': 2}"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3292, 1: 1333, 2: 3541})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(train_label['type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof f1 0.918953702108967\n"
     ]
    }
   ],
   "source": [
    "oof = np.argmax(oof, axis=1)\n",
    "print('oof f1', metrics.f1_score(oof, y, average='macro'))\n",
    "# 0.8701544575329372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.6350\n",
      "1    0.2355\n",
      "2    0.1295\n",
      "Name: pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(pred, axis=1)\n",
    "sub = test_label[['ship']]\n",
    "sub['pred'] = pred\n",
    "\n",
    "print(sub['pred'].value_counts(1))\n",
    "sub['pred'] = sub['pred'].map(type_map_rev)\n",
    "sub.to_csv('210-dart-r513-k7-result.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([train_label,train_label[train_label['type']==1]])[features].copy()\n",
    "y = pd.concat([train_label,train_label[train_label['type']==1]])[target]\n",
    "train_set = lgb.Dataset(X,y)\n",
    "params = {\n",
    "    'num_leaves':40,\n",
    "    'n_estimators': 1000,\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'multiclassova',\n",
    "    'num_class': 3,\n",
    "    'num_threads':20,\n",
    "}\n",
    "model1 = lgb.train(params, train_set)\n",
    "# test_pred = model.predict(test_label[features])\n",
    "# pred=np.argmax(test_pred, axis=1)\n",
    "# sub = test_label[['ship']]\n",
    "# sub['pred'] = pred\n",
    "# print(sub['pred'].value_counts(1))\n",
    "# sub['pred'] = sub['pred'].map(type_map_rev)\n",
    "# sub.to_csv('28-dart-1model-result.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_model('/home/sunnyu/yuchuan/sub/fusaisub/model/lgb_1.txt')\n",
    "pd.DataFrame(features).to_csv('/home/sunnyu/yuchuan/sub/fusaisub/model/features.csv',index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.save_model('/home/sunnyu/yuchuan/sub/best_model/lgb_1_89178.txt')\n",
    "# pd.DataFrame(features).to_csv('/home/sunnyu/yuchuan/sub/best_model/features.csv',index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>62</th>\n",
       "      <th>60</th>\n",
       "      <th>35</th>\n",
       "      <th>52</th>\n",
       "      <th>63</th>\n",
       "      <th>12</th>\n",
       "      <th>59</th>\n",
       "      <th>48</th>\n",
       "      <th>47</th>\n",
       "      <th>32</th>\n",
       "      <th>61</th>\n",
       "      <th>20</th>\n",
       "      <th>56</th>\n",
       "      <th>40</th>\n",
       "      <th>46</th>\n",
       "      <th>34</th>\n",
       "      <th>43</th>\n",
       "      <th>30</th>\n",
       "      <th>57</th>\n",
       "      <th>19</th>\n",
       "      <th>2</th>\n",
       "      <th>51</th>\n",
       "      <th>41</th>\n",
       "      <th>4</th>\n",
       "      <th>14</th>\n",
       "      <th>55</th>\n",
       "      <th>29</th>\n",
       "      <th>45</th>\n",
       "      <th>44</th>\n",
       "      <th>5</th>\n",
       "      <th>18</th>\n",
       "      <th>1</th>\n",
       "      <th>37</th>\n",
       "      <th>53</th>\n",
       "      <th>21</th>\n",
       "      <th>24</th>\n",
       "      <th>0</th>\n",
       "      <th>50</th>\n",
       "      <th>54</th>\n",
       "      <th>16</th>\n",
       "      <th>39</th>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <th>13</th>\n",
       "      <th>38</th>\n",
       "      <th>17</th>\n",
       "      <th>23</th>\n",
       "      <th>31</th>\n",
       "      <th>25</th>\n",
       "      <th>49</th>\n",
       "      <th>58</th>\n",
       "      <th>8</th>\n",
       "      <th>26</th>\n",
       "      <th>65</th>\n",
       "      <th>64</th>\n",
       "      <th>7</th>\n",
       "      <th>27</th>\n",
       "      <th>22</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>33</th>\n",
       "      <th>28</th>\n",
       "      <th>36</th>\n",
       "      <th>10</th>\n",
       "      <th>6</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>y_min</td>\n",
       "      <td>y_mean</td>\n",
       "      <td>x_0</td>\n",
       "      <td>y_0</td>\n",
       "      <td>y_mode</td>\n",
       "      <td>area</td>\n",
       "      <td>y_max</td>\n",
       "      <td>x_mode</td>\n",
       "      <td>x_min</td>\n",
       "      <td>v_median</td>\n",
       "      <td>y_median</td>\n",
       "      <td>diffd_kurt</td>\n",
       "      <td>y_25</td>\n",
       "      <td>x_25</td>\n",
       "      <td>x_median</td>\n",
       "      <td>v_std</td>\n",
       "      <td>x_max</td>\n",
       "      <td>v_75</td>\n",
       "      <td>y_75</td>\n",
       "      <td>diffd_75</td>\n",
       "      <td>20</td>\n",
       "      <td>x_sum</td>\n",
       "      <td>x_75</td>\n",
       "      <td>23</td>\n",
       "      <td>d_mean</td>\n",
       "      <td>y_0_y_min</td>\n",
       "      <td>slope</td>\n",
       "      <td>x_mean</td>\n",
       "      <td>x_max_x_min</td>\n",
       "      <td>26</td>\n",
       "      <td>diffd_25</td>\n",
       "      <td>19</td>\n",
       "      <td>x_0_x_max</td>\n",
       "      <td>y_0_y_max</td>\n",
       "      <td>diffd_std</td>\n",
       "      <td>diffv_mean</td>\n",
       "      <td>18</td>\n",
       "      <td>x_std</td>\n",
       "      <td>y_0_y_mean</td>\n",
       "      <td>d_sum</td>\n",
       "      <td>x_0_x_min</td>\n",
       "      <td>x_kurt</td>\n",
       "      <td>22</td>\n",
       "      <td>d_75</td>\n",
       "      <td>x_0_x_mean</td>\n",
       "      <td>diff_second</td>\n",
       "      <td>diffv_max</td>\n",
       "      <td>v_kurt</td>\n",
       "      <td>diffv_skew</td>\n",
       "      <td>x_skew</td>\n",
       "      <td>y_kurt</td>\n",
       "      <td>29</td>\n",
       "      <td>diffv_std</td>\n",
       "      <td>y_std</td>\n",
       "      <td>y_skew</td>\n",
       "      <td>28</td>\n",
       "      <td>diffy_25</td>\n",
       "      <td>diffv_kurt</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>v_skew</td>\n",
       "      <td>diffy_75</td>\n",
       "      <td>x_0_x_25</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>d_std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>16173.7</td>\n",
       "      <td>15662.3</td>\n",
       "      <td>11142.6</td>\n",
       "      <td>10142</td>\n",
       "      <td>9863.75</td>\n",
       "      <td>8647.69</td>\n",
       "      <td>6314.14</td>\n",
       "      <td>6278.51</td>\n",
       "      <td>5961.34</td>\n",
       "      <td>5810.51</td>\n",
       "      <td>5610.29</td>\n",
       "      <td>5562.56</td>\n",
       "      <td>5532.92</td>\n",
       "      <td>5458.44</td>\n",
       "      <td>4664.65</td>\n",
       "      <td>4602.79</td>\n",
       "      <td>4326.77</td>\n",
       "      <td>4259.48</td>\n",
       "      <td>4034.69</td>\n",
       "      <td>3633.61</td>\n",
       "      <td>3396.86</td>\n",
       "      <td>3161.74</td>\n",
       "      <td>3087.62</td>\n",
       "      <td>2706.36</td>\n",
       "      <td>2699.26</td>\n",
       "      <td>2655.09</td>\n",
       "      <td>2590.92</td>\n",
       "      <td>2582.83</td>\n",
       "      <td>2482.46</td>\n",
       "      <td>2450.8</td>\n",
       "      <td>2417.95</td>\n",
       "      <td>2361.21</td>\n",
       "      <td>2290.01</td>\n",
       "      <td>2251.8</td>\n",
       "      <td>2199.44</td>\n",
       "      <td>2168.54</td>\n",
       "      <td>2125.25</td>\n",
       "      <td>2064.31</td>\n",
       "      <td>2054.41</td>\n",
       "      <td>2014.38</td>\n",
       "      <td>1998.73</td>\n",
       "      <td>1957.05</td>\n",
       "      <td>1950.58</td>\n",
       "      <td>1857.96</td>\n",
       "      <td>1793.22</td>\n",
       "      <td>1756.07</td>\n",
       "      <td>1734.02</td>\n",
       "      <td>1676.02</td>\n",
       "      <td>1669.58</td>\n",
       "      <td>1608.44</td>\n",
       "      <td>1594.04</td>\n",
       "      <td>1530</td>\n",
       "      <td>1496.84</td>\n",
       "      <td>1489.16</td>\n",
       "      <td>1476.39</td>\n",
       "      <td>1473.05</td>\n",
       "      <td>1413.95</td>\n",
       "      <td>1402.9</td>\n",
       "      <td>1344.01</td>\n",
       "      <td>1289.42</td>\n",
       "      <td>1275.43</td>\n",
       "      <td>1264.09</td>\n",
       "      <td>1214.27</td>\n",
       "      <td>1211.88</td>\n",
       "      <td>1110.31</td>\n",
       "      <td>1086.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            62       60       35     52       63       12       59       48  \\\n",
       "name     y_min   y_mean      x_0    y_0   y_mode     area    y_max   x_mode   \n",
       "score  16173.7  15662.3  11142.6  10142  9863.75  8647.69  6314.14  6278.51   \n",
       "\n",
       "            47        32        61          20       56       40        46  \\\n",
       "name     x_min  v_median  y_median  diffd_kurt     y_25     x_25  x_median   \n",
       "score  5961.34   5810.51   5610.29     5562.56  5532.92  5458.44   4664.65   \n",
       "\n",
       "            34       43       30       57        19       2        51  \\\n",
       "name     v_std    x_max     v_75     y_75  diffd_75       20    x_sum   \n",
       "score  4602.79  4326.77  4259.48  4034.69   3633.61  3396.86  3161.74   \n",
       "\n",
       "            41       4        14         55       29       45           44  \\\n",
       "name      x_75       23   d_mean  y_0_y_min    slope   x_mean  x_max_x_min   \n",
       "score  3087.62  2706.36  2699.26    2655.09  2590.92  2582.83      2482.46   \n",
       "\n",
       "           5         18       1          37         53         21          24  \\\n",
       "name       26  diffd_25       19  x_0_x_max  y_0_y_max  diffd_std  diffv_mean   \n",
       "score  2450.8   2417.95  2361.21    2290.01     2251.8    2199.44     2168.54   \n",
       "\n",
       "            0        50          54       16         39       42       3   \\\n",
       "name        18    x_std  y_0_y_mean    d_sum  x_0_x_min   x_kurt       22   \n",
       "score  2125.25  2064.31     2054.41  2014.38    1998.73  1957.05  1950.58   \n",
       "\n",
       "            13          38           17         23       31          25  \\\n",
       "name      d_75  x_0_x_mean  diff_second  diffv_max   v_kurt  diffv_skew   \n",
       "score  1857.96     1793.22      1756.07    1734.02  1676.02     1669.58   \n",
       "\n",
       "            49       58    8          26       65       64       7         27  \\\n",
       "name    x_skew   y_kurt    29  diffv_std    y_std   y_skew       28  diffy_25   \n",
       "score  1608.44  1594.04  1530    1496.84  1489.16  1476.39  1473.05   1413.95   \n",
       "\n",
       "               22       9        11       33        28        36       10  \\\n",
       "name   diffv_kurt       31       35   v_skew  diffy_75  x_0_x_25       33   \n",
       "score      1402.9  1344.01  1289.42  1275.43   1264.09   1214.27  1211.88   \n",
       "\n",
       "            6        15  \n",
       "name        27    d_std  \n",
       "score  1110.31  1086.77  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for index, model in enumerate(models):\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = model.feature_name()\n",
    "    df['score'] = model.feature_importance(importance_type='gain')\n",
    "    df['fold'] = index\n",
    "    ret.append(df)\n",
    "    \n",
    "df = pd.concat(ret)\n",
    "df = df.groupby('name', as_index=False)['score'].mean()\n",
    "df = df.sort_values(['score'],  ascending=False)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predd=np.argmax(pred, axis=1)\n",
    "subb = test_label[['ship']]\n",
    "subb['p0'],subb['p1'],subb['p2'] = pred[:,0],pred[:,1],pred[:,2]\n",
    "subb=subb.sort_values('ship')\n",
    "subb.to_csv('model1_p.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel2=pd.read_csv('~/yuchuan/baseline/tianchi_ship_2019/working/model2_p.csv')\n",
    "submodel1=subb.reset_index()\n",
    "\n",
    "summodel2=(submodel2['p0']+submodel2['p1']+submodel2['p2'])\n",
    "summodel1=(submodel1['p0']+submodel1['p1']+submodel1['p2'])\n",
    "\n",
    "final_sub=pd.DataFrame()\n",
    "final_sub['p0']=submodel1['p0']/summodel1+submodel2['p0']/summodel2\n",
    "final_sub['p1']=submodel1['p1']/summodel1+submodel2['p1']/summodel2\n",
    "final_sub['p2']=submodel1['p2']/summodel1+submodel2['p2']/summodel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.639\n",
      "1    0.229\n",
      "2    0.132\n",
      "Name: pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "final_pred = np.argmax(final_sub.values, axis=1)\n",
    "sub = submodel1[['ship']]\n",
    "sub['pred'] = final_pred\n",
    "\n",
    "print(sub['pred'].value_counts(1))\n",
    "sub['pred'] = sub['pred'].map(type_map_rev)\n",
    "sub.to_csv('210-2lgb-result.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features=set(features) | set(del_f_ga)\n",
    "len(final_features),np.array(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth':7,\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'num_threads':20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.085391\tval-merror:0.154966\ttrain-f1-score:0.111723\tval-f1-score:0.20642\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.096747\ttrain-f1-score:0\tval-f1-score:0.125165\n",
      "[200]\ttrain-merror:0\tval-merror:0.103596\ttrain-f1-score:0\tval-f1-score:0.134165\n",
      "[300]\ttrain-merror:0\tval-merror:0.100171\ttrain-f1-score:0\tval-f1-score:0.129408\n",
      "[400]\ttrain-merror:0\tval-merror:0.10274\ttrain-f1-score:0\tval-f1-score:0.133132\n",
      "[500]\ttrain-merror:0\tval-merror:0.103596\ttrain-f1-score:0\tval-f1-score:0.13435\n",
      "[600]\ttrain-merror:0\tval-merror:0.10274\ttrain-f1-score:0\tval-f1-score:0.133606\n",
      "Stopping. Best iteration:\n",
      "[100]\ttrain-merror:0\tval-merror:0.096747\ttrain-f1-score:0\tval-f1-score:0.125165\n",
      "\n",
      "[0]\ttrain-merror:0.092405\tval-merror:0.158526\ttrain-f1-score:0.125213\tval-f1-score:0.210038\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.083119\ttrain-f1-score:0\tval-f1-score:0.104872\n",
      "[200]\ttrain-merror:0\tval-merror:0.079692\ttrain-f1-score:0\tval-f1-score:0.102238\n",
      "[300]\ttrain-merror:0\tval-merror:0.080548\ttrain-f1-score:0\tval-f1-score:0.103049\n",
      "[400]\ttrain-merror:0\tval-merror:0.077121\ttrain-f1-score:0\tval-f1-score:0.099852\n",
      "[500]\ttrain-merror:0\tval-merror:0.077978\ttrain-f1-score:0\tval-f1-score:0.100395\n",
      "Stopping. Best iteration:\n",
      "[68]\ttrain-merror:0\tval-merror:0.077978\ttrain-f1-score:0\tval-f1-score:0.097365\n",
      "\n",
      "[0]\ttrain-merror:0.100463\tval-merror:0.159383\ttrain-f1-score:0.135983\tval-f1-score:0.206055\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.098543\ttrain-f1-score:0\tval-f1-score:0.12925\n",
      "[200]\ttrain-merror:0\tval-merror:0.0994\ttrain-f1-score:0\tval-f1-score:0.128687\n",
      "[300]\ttrain-merror:0\tval-merror:0.094259\ttrain-f1-score:0\tval-f1-score:0.123306\n",
      "[400]\ttrain-merror:0\tval-merror:0.094259\ttrain-f1-score:0\tval-f1-score:0.122647\n",
      "[500]\ttrain-merror:0\tval-merror:0.097686\ttrain-f1-score:0\tval-f1-score:0.127674\n",
      "[600]\ttrain-merror:0\tval-merror:0.0994\ttrain-f1-score:0\tval-f1-score:0.129818\n",
      "[700]\ttrain-merror:0\tval-merror:0.098543\ttrain-f1-score:0\tval-f1-score:0.129072\n",
      "[800]\ttrain-merror:0\tval-merror:0.0994\ttrain-f1-score:0\tval-f1-score:0.129818\n",
      "Stopping. Best iteration:\n",
      "[340]\ttrain-merror:0\tval-merror:0.090831\ttrain-f1-score:0\tval-f1-score:0.117915\n",
      "\n",
      "[0]\ttrain-merror:0.091891\tval-merror:0.169666\ttrain-f1-score:0.118333\tval-f1-score:0.218267\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.108826\ttrain-f1-score:0\tval-f1-score:0.138587\n",
      "[200]\ttrain-merror:0\tval-merror:0.106255\ttrain-f1-score:0\tval-f1-score:0.136439\n",
      "[300]\ttrain-merror:0\tval-merror:0.105398\ttrain-f1-score:0\tval-f1-score:0.135515\n",
      "[400]\ttrain-merror:0\tval-merror:0.107112\ttrain-f1-score:0\tval-f1-score:0.137001\n",
      "[500]\ttrain-merror:0\tval-merror:0.106255\ttrain-f1-score:0\tval-f1-score:0.13608\n",
      "[600]\ttrain-merror:0\tval-merror:0.106255\ttrain-f1-score:0\tval-f1-score:0.135906\n",
      "Stopping. Best iteration:\n",
      "[176]\ttrain-merror:0\tval-merror:0.104542\ttrain-f1-score:0\tval-f1-score:0.134113\n",
      "\n",
      "[0]\ttrain-merror:0.093932\tval-merror:0.171527\ttrain-f1-score:0.124138\tval-f1-score:0.229362\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.089194\ttrain-f1-score:0\tval-f1-score:0.118298\n",
      "[200]\ttrain-merror:0\tval-merror:0.088336\ttrain-f1-score:0\tval-f1-score:0.116321\n",
      "[300]\ttrain-merror:0\tval-merror:0.088336\ttrain-f1-score:0\tval-f1-score:0.115579\n",
      "[400]\ttrain-merror:0\tval-merror:0.089194\ttrain-f1-score:0\tval-f1-score:0.117084\n",
      "[500]\ttrain-merror:0\tval-merror:0.090051\ttrain-f1-score:0\tval-f1-score:0.118771\n",
      "[600]\ttrain-merror:0\tval-merror:0.089194\ttrain-f1-score:0\tval-f1-score:0.117863\n",
      "[700]\ttrain-merror:0\tval-merror:0.089194\ttrain-f1-score:0\tval-f1-score:0.117232\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-merror:0\tval-merror:0.086621\ttrain-f1-score:0\tval-f1-score:0.113416\n",
      "\n",
      "[0]\ttrain-merror:0.104199\tval-merror:0.162232\ttrain-f1-score:0.144241\tval-f1-score:0.227548\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.091845\ttrain-f1-score:0\tval-f1-score:0.119534\n",
      "[200]\ttrain-merror:0\tval-merror:0.090129\ttrain-f1-score:0\tval-f1-score:0.118101\n",
      "[300]\ttrain-merror:0\tval-merror:0.090987\ttrain-f1-score:0\tval-f1-score:0.119016\n",
      "[400]\ttrain-merror:0\tval-merror:0.090129\ttrain-f1-score:0\tval-f1-score:0.117415\n",
      "[500]\ttrain-merror:0\tval-merror:0.091845\ttrain-f1-score:0\tval-f1-score:0.119264\n",
      "Stopping. Best iteration:\n",
      "[56]\ttrain-merror:0\tval-merror:0.086695\ttrain-f1-score:0\tval-f1-score:0.111969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mac_f1_xgb(predss,mm):\n",
    "    labell=mm.get_label()\n",
    "    predss=np.argmax(predss.reshape(len(labell),3), axis=1)\n",
    "    f11=metrics.f1_score(labell,predss,average='macro')\n",
    "    return 'f1-score',1-f11\n",
    "\n",
    "fold = StratifiedKFold(n_splits=6, shuffle=True, random_state=1)\n",
    "\n",
    "X = train_label[features].copy()\n",
    "y = train_label[target]\n",
    "models = []\n",
    "i=0\n",
    "\n",
    "pred = np.zeros((len(test_label),3))\n",
    "oof = np.zeros((len(X), 3))\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n",
    "    train_set  = xgb.DMatrix(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    val_set  = xgb.DMatrix(X.iloc[val_idx], y.iloc[val_idx])\n",
    "    watchlist = [(train_set,'train'),(val_set,'val')]\n",
    "    i+=1\n",
    "    model = xgb.train(param,train_set, num_boost_round=10000, early_stopping_rounds=500,evals=watchlist,feval=mac_f1_xgb, verbose_eval=100)\n",
    "    models.append(model)\n",
    "\n",
    "    test_pred = model.predict(xgb.DMatrix(test_label[features]))\n",
    "    pred += test_pred/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, array(['x_0_x_max', 'y_0_y_median', '8', 'x_0_x_mean', '2',\n",
       "        'diffv_median', 'diffd_std', 'd_75', '6', 'x_kurt', '3', 'v_max',\n",
       "        'd_median', '1', '26', '8_cnn', 'x_0_x_mode', 'x_0_x_median',\n",
       "        'diffx_25', 'diffx_75', '5_cnn', 'd_max', 'x_0_x_25', 'y_0_y_25',\n",
       "        '20', 'd_mean', 'x_skew', 'd_25', 'y_max_x_max', 'diffd_mode',\n",
       "        'y_0_y_75', 'diff_day', 'x_75', 'diffy_25', 'diffy_mode',\n",
       "        'y_max_y_min', 'x_mean', 'v_25', 'v_std', 'x_max', 'y_0_y_mode',\n",
       "        'x_mode', 'x_min_y_min', 'y_max', 'diffx_mode', 'x_25', 'diffy_75',\n",
       "        'diffx_median', 'diffy_median', 'y', 'x_min', 'diffd_25', 'y_25',\n",
       "        'y_max_x_min', 'y_0', 'y_75', 'v_75', 'x_max_x_min', 'x',\n",
       "        'diffd_75', 'v_min', 'x_0', 'v_median', 'diffd_kurt', 'area',\n",
       "        'x_median', 'x_max_y_min', 'y_median', 'y_mode', 'y_min', 'y_mean'],\n",
       "       dtype='<U12'))"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = {}\n",
    "from collections import Counter\n",
    "for index, model in enumerate(models):\n",
    "    Model,Z=Counter(model.get_score(importance_type='gain')),Counter(z)\n",
    "    z=dict(Model+Z)\n",
    "\n",
    "    \n",
    "df_xgb=pd.Series(z).reset_index().sort_values(0)\n",
    "xgb_features=df_xgb[df_xgb[0]>3]['index'].values.tolist()\n",
    "len(xgb_features),np.array(xgb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth':5,\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 3,\n",
    "    'num_threads':20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.166324\tval-merror:0.196062\ttrain-f1-score:0.236415\tval-f1-score:0.272588\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.086473\ttrain-f1-score:0\tval-f1-score:0.113288\n",
      "[200]\ttrain-merror:0\tval-merror:0.081336\ttrain-f1-score:0\tval-f1-score:0.104929\n",
      "[300]\ttrain-merror:0\tval-merror:0.083904\ttrain-f1-score:0\tval-f1-score:0.108672\n",
      "[400]\ttrain-merror:0\tval-merror:0.083048\ttrain-f1-score:0\tval-f1-score:0.107902\n",
      "[500]\ttrain-merror:0\tval-merror:0.083048\ttrain-f1-score:0\tval-f1-score:0.107748\n",
      "[600]\ttrain-merror:0\tval-merror:0.082192\ttrain-f1-score:0\tval-f1-score:0.106978\n",
      "Stopping. Best iteration:\n",
      "[119]\ttrain-merror:0\tval-merror:0.079623\ttrain-f1-score:0\tval-f1-score:0.103034\n",
      "\n",
      "[0]\ttrain-merror:0.167152\tval-merror:0.200514\ttrain-f1-score:0.216015\tval-f1-score:0.262214\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.076264\ttrain-f1-score:0\tval-f1-score:0.104797\n",
      "[200]\ttrain-merror:0\tval-merror:0.071979\ttrain-f1-score:0\tval-f1-score:0.096026\n",
      "[300]\ttrain-merror:0\tval-merror:0.071979\ttrain-f1-score:0\tval-f1-score:0.097173\n",
      "[400]\ttrain-merror:0\tval-merror:0.068552\ttrain-f1-score:0\tval-f1-score:0.093588\n",
      "[500]\ttrain-merror:0\tval-merror:0.069409\ttrain-f1-score:0\tval-f1-score:0.094355\n",
      "[600]\ttrain-merror:0\tval-merror:0.071123\ttrain-f1-score:0\tval-f1-score:0.096268\n",
      "[700]\ttrain-merror:0\tval-merror:0.070266\ttrain-f1-score:0\tval-f1-score:0.094599\n",
      "[800]\ttrain-merror:0\tval-merror:0.071979\ttrain-f1-score:0\tval-f1-score:0.096283\n",
      "Stopping. Best iteration:\n",
      "[357]\ttrain-merror:0\tval-merror:0.068552\ttrain-f1-score:0\tval-f1-score:0.093149\n",
      "\n",
      "[0]\ttrain-merror:0.15618\tval-merror:0.193659\ttrain-f1-score:0.201565\tval-f1-score:0.250951\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0.000171\tval-merror:0.098543\ttrain-f1-score:0.000243\tval-f1-score:0.127684\n",
      "[200]\ttrain-merror:0\tval-merror:0.096829\ttrain-f1-score:0\tval-f1-score:0.12588\n",
      "[300]\ttrain-merror:0\tval-merror:0.096829\ttrain-f1-score:0\tval-f1-score:0.124015\n",
      "[400]\ttrain-merror:0\tval-merror:0.095116\ttrain-f1-score:0\tval-f1-score:0.121807\n",
      "[500]\ttrain-merror:0\tval-merror:0.094259\ttrain-f1-score:0\tval-f1-score:0.121587\n",
      "[600]\ttrain-merror:0\tval-merror:0.094259\ttrain-f1-score:0\tval-f1-score:0.121861\n",
      "[700]\ttrain-merror:0\tval-merror:0.093402\ttrain-f1-score:0\tval-f1-score:0.120951\n",
      "[800]\ttrain-merror:0\tval-merror:0.093402\ttrain-f1-score:0\tval-f1-score:0.120951\n",
      "[900]\ttrain-merror:0\tval-merror:0.093402\ttrain-f1-score:0\tval-f1-score:0.120951\n",
      "Stopping. Best iteration:\n",
      "[485]\ttrain-merror:0\tval-merror:0.092545\ttrain-f1-score:0\tval-f1-score:0.118943\n",
      "\n",
      "[0]\ttrain-merror:0.164924\tval-merror:0.181662\ttrain-f1-score:0.224973\tval-f1-score:0.234631\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.089974\ttrain-f1-score:0\tval-f1-score:0.112045\n",
      "[200]\ttrain-merror:0\tval-merror:0.081405\ttrain-f1-score:0\tval-f1-score:0.100523\n",
      "[300]\ttrain-merror:0\tval-merror:0.078835\ttrain-f1-score:0\tval-f1-score:0.096008\n",
      "[400]\ttrain-merror:0\tval-merror:0.081405\ttrain-f1-score:0\tval-f1-score:0.099243\n",
      "[500]\ttrain-merror:0\tval-merror:0.080548\ttrain-f1-score:0\tval-f1-score:0.098122\n",
      "[600]\ttrain-merror:0\tval-merror:0.081405\ttrain-f1-score:0\tval-f1-score:0.098893\n",
      "[700]\ttrain-merror:0\tval-merror:0.081405\ttrain-f1-score:0\tval-f1-score:0.099201\n",
      "[800]\ttrain-merror:0\tval-merror:0.080548\ttrain-f1-score:0\tval-f1-score:0.098075\n",
      "Stopping. Best iteration:\n",
      "[311]\ttrain-merror:0\tval-merror:0.076264\ttrain-f1-score:0\tval-f1-score:0.092177\n",
      "\n",
      "[0]\ttrain-merror:0.168666\tval-merror:0.180103\ttrain-f1-score:0.244051\tval-f1-score:0.255631\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0\tval-merror:0.082333\ttrain-f1-score:0\tval-f1-score:0.102096\n",
      "[200]\ttrain-merror:0\tval-merror:0.092624\ttrain-f1-score:0\tval-f1-score:0.115806\n",
      "[300]\ttrain-merror:0\tval-merror:0.092624\ttrain-f1-score:0\tval-f1-score:0.111899\n",
      "[400]\ttrain-merror:0\tval-merror:0.091767\ttrain-f1-score:0\tval-f1-score:0.110979\n",
      "[500]\ttrain-merror:0\tval-merror:0.088336\ttrain-f1-score:0\tval-f1-score:0.105769\n",
      "[600]\ttrain-merror:0\tval-merror:0.080617\ttrain-f1-score:0\tval-f1-score:0.095746\n",
      "[700]\ttrain-merror:0\tval-merror:0.08319\ttrain-f1-score:0\tval-f1-score:0.099182\n",
      "[800]\ttrain-merror:0\tval-merror:0.08319\ttrain-f1-score:0\tval-f1-score:0.098785\n",
      "[900]\ttrain-merror:0\tval-merror:0.08319\ttrain-f1-score:0\tval-f1-score:0.098637\n",
      "[1000]\ttrain-merror:0\tval-merror:0.08319\ttrain-f1-score:0\tval-f1-score:0.098637\n",
      "Stopping. Best iteration:\n",
      "[559]\ttrain-merror:0\tval-merror:0.080617\ttrain-f1-score:0\tval-f1-score:0.095746\n",
      "\n",
      "[0]\ttrain-merror:0.155955\tval-merror:0.201717\ttrain-f1-score:0.214111\tval-f1-score:0.273872\n",
      "Multiple eval metrics have been passed: 'val-f1-score' will be used for early stopping.\n",
      "\n",
      "Will train until val-f1-score hasn't improved in 500 rounds.\n",
      "[100]\ttrain-merror:0.000171\tval-merror:0.095279\ttrain-f1-score:0.000242\tval-f1-score:0.124928\n",
      "[200]\ttrain-merror:0\tval-merror:0.090987\ttrain-f1-score:0\tval-f1-score:0.113298\n",
      "[300]\ttrain-merror:0\tval-merror:0.088412\ttrain-f1-score:0\tval-f1-score:0.111322\n",
      "[400]\ttrain-merror:0\tval-merror:0.088412\ttrain-f1-score:0\tval-f1-score:0.111537\n",
      "[500]\ttrain-merror:0\tval-merror:0.090129\ttrain-f1-score:0\tval-f1-score:0.114188\n",
      "[600]\ttrain-merror:0\tval-merror:0.092704\ttrain-f1-score:0\tval-f1-score:0.115909\n",
      "[700]\ttrain-merror:0\tval-merror:0.090987\ttrain-f1-score:0\tval-f1-score:0.114225\n",
      "Stopping. Best iteration:\n",
      "[272]\ttrain-merror:0\tval-merror:0.086695\ttrain-f1-score:0\tval-f1-score:0.109797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mac_f1_xgb(predss,mm):\n",
    "    labell=mm.get_label()\n",
    "    predss=np.argmax(predss.reshape(len(labell),3), axis=1)\n",
    "    f11=metrics.f1_score(labell,predss,average='macro')\n",
    "    return 'f1-score',1-f11\n",
    "\n",
    "fold = StratifiedKFold(n_splits=6, shuffle=True, random_state=513)\n",
    "\n",
    "X = train_label[xgb_features].copy()\n",
    "y = train_label[target]\n",
    "models = []\n",
    "i=0\n",
    "\n",
    "pred = np.zeros((len(test_label),3))\n",
    "oof = np.zeros((len(X), 3))\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n",
    "    train_set  = xgb.DMatrix(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    val_set  = xgb.DMatrix(X.iloc[val_idx], y.iloc[val_idx])\n",
    "    watchlist = [(train_set,'train'),(val_set,'val')]\n",
    "    i+=1\n",
    "    model = xgb.train(param,train_set, num_boost_round=10000, early_stopping_rounds=500,evals=watchlist,feval=mac_f1_xgb, verbose_eval=100)\n",
    "    models.append(model)\n",
    "\n",
    "    test_pred = model.predict(xgb.DMatrix(test_label[xgb_features]),ntree_limit=model.best_ntree_limit)\n",
    "    pred += test_pred/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.6500\n",
      "1    0.2315\n",
      "2    0.1185\n",
      "Name: pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(pred, axis=1)\n",
    "sub = test_label[['ship']]\n",
    "sub['pred'] = pred\n",
    "\n",
    "print(sub['pred'].value_counts(1))\n",
    "sub['pred'] = sub['pred'].map(type_map_rev)\n",
    "sub.to_csv('215-xgb-r513-k6-result.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999416e-01, 1.6157942e-06, 4.1886774e-06],\n",
       "       [9.9999607e-01, 3.5575163e-06, 3.7250729e-07],\n",
       "       [9.9967515e-01, 3.1354115e-04, 1.1280426e-05],\n",
       "       ...,\n",
       "       [9.9917221e-01, 8.0860383e-04, 1.9268904e-05],\n",
       "       [3.6292691e-02, 3.2079753e-01, 6.4290977e-01],\n",
       "       [1.5105685e-04, 9.9934143e-01, 5.0752575e-04]], dtype=float32)"
      ]
     },
     "execution_count": 984,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(xgb.DMatrix(test_label[xgb_features]),ntree_limit=model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 977,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_ntree_limit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
