{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_hdf('/home/sunnyu/yuchuan/data/train.h5')\n",
    "test = pd.read_hdf('/home/sunnyu/yuchuan/data/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(train):\n",
    "    m,n=4,4\n",
    "    df=train\n",
    "    ymax,ymin,xmax,xmin=df['y'].max(),df['y'].min(),df['x'].max(),df['x'].min()\n",
    "    yy=(df['y']-ymin)/(ymax-ymin)\n",
    "    xx=(df['x']-xmin)/(xmax-xmin)\n",
    "    count,count10,vmean,dmean=[],[],[],[]\n",
    "    for i in range(m):\n",
    "        count.append([]),count10.append([]),vmean.append([]),dmean.append([])\n",
    "        for j in range(n):\n",
    "            if ((i==m)|(j==n)):\n",
    "                dfij=df[((i/m)<=yy) & (yy<=((i+1)/m)) & ((j/n)<=xx) & (xx<=((j+1)/n))]\n",
    "            else:\n",
    "                dfij=df[((i/m)<=yy) & (yy<((i+1)/m)) & ((j/n)<=xx) & (xx<((j+1)/n))]\n",
    "            cc=dfij.shape[0]\n",
    "            count[i].append(cc)\n",
    "            count10[i].append(cc!=0)\n",
    "            vmean[i].append(dfij['v'].mean())\n",
    "            dmean[i].append(dfij['d'].mean())\n",
    "    map_f=np.array([np.array(count),np.array(count10),np.array(vmean),np.array(dmean)]).reshape(m*n*4).T\n",
    "    return pd.DataFrame(map_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p=train.groupby(['ship']).apply(map_feature).unstack()\n",
    "test_p=test.groupby(['ship']).apply(map_feature).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p.to_hdf('/home/sunnyu/yuchuan/data/train_cnn_4.h5', 'df', mode='w')\n",
    "test_p.to_hdf('/home/sunnyu/yuchuan/data/test_cnn_4.h5', 'df', mode='w')\n",
    "del train_p,test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn = pd.read_hdf('/home/sunnyu/yuchuan/data/train_cnn_4.h5')\n",
    "test_cnn = pd.read_hdf('/home/sunnyu/yuchuan/data/test_cnn_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn.fillna(0,inplace=True)\n",
    "test_cnn.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.groupby(['ship'])['type'].first()\n",
    "type_map = dict(zip(y_train.unique(), np.arange(3)))\n",
    "y_train=y_train.map(type_map)\n",
    "type_map_rev = {v:k for k,v in type_map.items()}\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_cnn,y_train, \n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "y_train=torch.tensor(y_train.values)\n",
    "x_train=torch.Tensor(x_train.values).reshape(5600,4,4,4)\n",
    "\n",
    "y_val=torch.tensor(y_val.values)\n",
    "x_val=torch.Tensor(x_val.values).reshape(1400,4,4,4)\n",
    "\n",
    "\n",
    "# x_train=torch.cat((x_train,\n",
    "#                    x_train[y_train==1],x_train[y_train==1][:914],\n",
    "#                    x_train[y_train==2],x_train[y_train==2],\n",
    "#                    x_train[y_train==2],x_train[y_train==2][:216]),axis=0)\n",
    "# y_train=torch.cat((y_train,\n",
    "#                    y_train[y_train==1],y_train[y_train==1][:914],\n",
    "#                    y_train[y_train==2],y_train[y_train==2],\n",
    "#                    y_train[y_train==2],y_train[y_train==2][:216]),axis=0)\n",
    "\n",
    "train_data=torch.utils.data.TensorDataset(x_train,y_train)\n",
    "validation_data=torch.utils.data.TensorDataset(x_val,y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data , batch_size=1400)\n",
    "\n",
    "x_test=torch.Tensor(test_cnn.values).reshape(2000,4,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm2d-1              [-1, 4, 4, 4]               8\n",
      "            Conv2d-2             [-1, 16, 1, 1]           1,040\n",
      "              ReLU-3             [-1, 16, 1, 1]               0\n",
      "            Linear-4                    [-1, 3]              51\n",
      "================================================================\n",
      "Total params: 1,099\n",
      "Trainable params: 1,099\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(4,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train.groupby(['ship'])['type'].first()\n",
    "type_map = dict(zip(y_train.unique(), np.arange(3)))\n",
    "y_train=y_train.map(type_map)\n",
    "type_map_rev = {v:k for k,v in type_map.items()}\n",
    "\n",
    "\n",
    "y_train=torch.tensor(y_train.values)\n",
    "x_train=torch.Tensor(train_cnn.values).reshape(7000,4,4,4)\n",
    "\n",
    "train_data=torch.utils.data.TensorDataset(x_train,y_train)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128)\n",
    "\n",
    "x_test=torch.Tensor(test_cnn.values).reshape(2000,4,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.Conv2d(4, 16, 4, 1,),\n",
    "            nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2) \n",
    "        )\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(8, 16, 3),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        self.fc1 = nn.Linear(16, 3)\n",
    " \n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        per_out=[]\n",
    "        x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        per_out.append(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x,per_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "model =  Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004378601849079132\n",
      "0.004371167957782745\n",
      "0.004370763229472297\n",
      "0.004370439308030265\n",
      "0.004370149944509779\n",
      "0.004369944836412157\n",
      "0.004369686267205647\n",
      "0.004369599802153451\n",
      "0.004369369434458869\n",
      "0.004369258591106959\n",
      "0.0043691123723983765\n",
      "0.004369019333805357\n",
      "0.0043688625693321225\n",
      "0.004368752517870494\n",
      "0.004368657056774412\n",
      "0.004368539090667452\n",
      "0.004368401442255292\n",
      "0.004368361788136619\n",
      "0.004368214751992907\n",
      "0.004368164509534836\n",
      "0.004368018350430897\n",
      "0.004367974098239626\n",
      "0.004367852934769221\n",
      "0.004367781132459641\n",
      "0.004367672430617468\n",
      "0.004367612787655422\n",
      "0.004367513294730867\n",
      "0.0043674573429993225\n",
      "0.004367373492036547\n",
      "0.004367219367197582\n",
      "0.004367187474455152\n",
      "0.004367042950221471\n",
      "0.00436713576742581\n",
      "0.004366919862372535\n",
      "0.004366918453148433\n",
      "0.00436675175172942\n",
      "0.004366788868393217\n",
      "0.004366687438317708\n",
      "0.0043665571340492796\n",
      "0.004366531423160008\n",
      "0.00436651108946119\n",
      "0.004366346738168172\n",
      "0.004366317246641432\n",
      "0.004366282497133528\n",
      "0.004366165663514819\n",
      "0.004366077959537506\n",
      "0.004366077555077417\n",
      "0.004366015046834946\n",
      "0.004365921578236989\n",
      "0.004365837803908756\n",
      "0.004365754996027265\n",
      "0.0043657439947128295\n",
      "0.004365644569907869\n",
      "0.004365599142653601\n",
      "0.004365510783025196\n",
      "0.004365503362246922\n",
      "0.0043654203244618\n",
      "0.0043653310239315036\n",
      "0.004365263083151409\n",
      "0.004365241919245038\n",
      "0.00436514921273504\n",
      "0.004365090378693172\n",
      "0.004365009158849716\n",
      "0.004364977657794952\n",
      "0.0043649193644523625\n",
      "0.004364818083388465\n",
      "0.004364700347185135\n",
      "0.004364747064454215\n",
      "0.004364674555403846\n",
      "0.004364523623670851\n",
      "0.004364486366510391\n",
      "0.004364441220249448\n",
      "0.0043643627677645005\n",
      "0.004364333816937038\n",
      "0.004364241361618042\n",
      "0.004364190348557064\n",
      "0.004364057157720838\n",
      "0.004364124821765082\n",
      "0.004364009252616337\n",
      "0.00436396404675075\n",
      "0.00436384910770825\n",
      "0.004363826866660799\n",
      "0.004363801641123636\n",
      "0.004363627838236945\n",
      "0.004363695106336048\n",
      "0.004363538690975734\n",
      "0.004363528392144612\n",
      "0.004363432573420661\n",
      "0.004363428443670273\n",
      "0.00436329700265612\n",
      "0.004363303031240191\n",
      "0.004363230735063553\n",
      "0.004363138735294342\n",
      "0.004363061164106641\n",
      "0.004363007123981203\n",
      "0.004363018316881997\n",
      "0.004362879608358656\n",
      "0.00436278166941234\n",
      "0.004362864085606166\n",
      "0.0043627199786049975\n",
      "0.004362659100975309\n",
      "0.004362606342349733\n",
      "0.004362570928675788\n",
      "0.0043623994759150915\n",
      "0.004362475267478398\n",
      "0.004362357045922961\n",
      "0.004362266072205135\n",
      "0.00436221689411572\n",
      "0.00436214360169002\n",
      "0.004362106229577746\n",
      "0.0043620897233486175\n",
      "0.004362008052212852\n",
      "0.004361863161836351\n",
      "0.004361928782292775\n",
      "0.004361837612731116\n",
      "0.004361679971218109\n",
      "0.004361720034054347\n",
      "0.0043616756328514646\n",
      "0.0043614967082227975\n",
      "0.004361511694533485\n",
      "0.004361435885940279\n",
      "0.004361440126385008\n",
      "0.004361335243497576\n",
      "0.004361296764441899\n",
      "0.004361134529113769\n",
      "0.004361187406948634\n",
      "0.004361113339662552\n",
      "0.004361069764409747\n",
      "0.004360995092562266\n",
      "0.004360872566699982\n",
      "0.004360915890761785\n",
      "0.004360885956457683\n",
      "0.004360767539058413\n",
      "0.004360714154584067\n",
      "0.004360628217458725\n",
      "0.004360602983406612\n",
      "0.0043605180127280095\n",
      "0.004360492710556303\n",
      "0.00436050478049687\n",
      "0.00436041305746351\n",
      "0.0043603449548993796\n",
      "0.004360259882041386\n",
      "0.0043601799607276915\n",
      "0.004360142592872892\n",
      "0.004360167818410056\n",
      "0.004360032417944499\n",
      "0.004360053087983813\n",
      "0.004359893960612161\n",
      "0.0043598694673606326\n",
      "0.004359811212335314\n",
      "0.004359755379813058\n",
      "0.0043597489510263715\n",
      "0.004359651152576719\n",
      "0.004359612733125687\n",
      "0.004359598351376397\n",
      "0.004359507224389485\n",
      "0.004359438180923462\n",
      "0.004359300174883434\n",
      "0.004359392953770501\n",
      "0.0043592737913131716\n",
      "0.004359224847384861\n",
      "0.004359181050743376\n",
      "0.004359088982854571\n",
      "0.004359116341386523\n",
      "0.004358990085976465\n",
      "0.00435899133341653\n",
      "0.0043589277608054025\n",
      "0.004358863966805594\n",
      "0.004358805686235428\n",
      "0.0043587211455617635\n",
      "0.004358683973550797\n",
      "0.004358668510402952\n",
      "0.004358567982912063\n",
      "0.004358617603778839\n",
      "0.0043584187030792235\n",
      "0.004358396121433803\n",
      "0.004358422045196805\n",
      "0.0043583272056920185\n",
      "0.004358277303831918\n",
      "0.004358229121991566\n",
      "0.004358241179159709\n",
      "0.004358153266566141\n",
      "0.0043580413120133535\n",
      "0.0043581034413405825\n",
      "0.004357944590704782\n",
      "0.004357978748423713\n",
      "0.004357850862400872\n",
      "0.004357827216386795\n",
      "0.0043578117872987475\n",
      "0.004357745983770915\n",
      "0.004357670281614576\n",
      "0.004357600463288171\n",
      "0.0043576239815780096\n",
      "0.0043575416633061\n",
      "0.004357466863734382\n",
      "0.004357463351317814\n",
      "0.0043573955765792306\n",
      "0.004357294457299369\n",
      "0.004357269998107638\n",
      "0.004357258609363011\n",
      "0.004357266851833888\n",
      "0.0043571857426847735\n",
      "0.0043570884040423806\n",
      "0.0043570525859083446\n",
      "0.0043570301958492826\n",
      "0.004356929553406579\n",
      "0.004356984202350889\n",
      "0.004356870655502592\n",
      "0.00435689731155123\n",
      "0.004356770536729268\n",
      "0.004356744068009513\n",
      "0.004356695392302105\n",
      "0.004356608062982559\n",
      "0.00435666835308075\n",
      "0.004356549654688154\n",
      "0.004356568804809026\n",
      "0.004356436376060758\n",
      "0.004356394333498818\n",
      "0.004356391808816365\n",
      "0.004356318226882389\n",
      "0.00435636653644698\n",
      "0.004356206255299705\n",
      "0.004356157941477639\n",
      "0.004356088510581425\n",
      "0.004356114174638476\n",
      "0.0043559352329799106\n",
      "0.0043559738525322505\n",
      "0.004355880686214992\n",
      "0.004355878523417882\n",
      "0.004355775730950492\n",
      "0.0043557812869548795\n",
      "0.0043556802996567315\n",
      "0.004355618344885962\n",
      "0.00435557108266013\n",
      "0.004355485081672669\n",
      "0.004355447266783033\n",
      "0.004355360763413566\n",
      "0.004355343971933637\n",
      "0.004355210508619036\n",
      "0.004355195394584111\n",
      "0.004355069888489587\n",
      "0.004355119628565652\n",
      "0.004354985173259463\n",
      "0.00435499473129\n",
      "0.004354851871728897\n",
      "0.004354837098291942\n",
      "0.004354779243469238\n",
      "0.004354736817734582\n",
      "0.004354671150445938\n",
      "0.00435449241740363\n",
      "0.004354639147009168\n",
      "0.0043544558797563825\n",
      "0.0043544308500630515\n",
      "0.004354369516883578\n",
      "0.0043543105721473695\n",
      "0.004354261611189161\n",
      "0.004354252696037292\n",
      "0.0043541403668267385\n",
      "0.004354084491729736\n",
      "0.004354116410017013\n",
      "0.004354024921144758\n",
      "0.004353902786970138\n",
      "0.004353902224983488\n",
      "0.004353887970958438\n",
      "0.004353813086237226\n",
      "0.004353706070355006\n",
      "0.004353742735726493\n",
      "0.004353666292769568\n",
      "0.0043536036695752825\n",
      "0.004353497317859105\n",
      "0.004353476111377989\n",
      "0.004353404428277697\n",
      "0.004353430752243315\n",
      "0.004353221416473389\n",
      "0.004353365681001118\n",
      "0.004353202581405639\n",
      "0.00435314730661256\n",
      "0.0043530698333467755\n",
      "0.004353077607495444\n",
      "0.004352983891963959\n",
      "0.004352964890854699\n",
      "0.004352905584233148\n",
      "0.004352830222674778\n",
      "0.0043528116600854054\n",
      "0.004352677353790828\n",
      "0.004352689176797867\n",
      "0.004352647402456829\n",
      "0.004352518277508872\n",
      "0.004352504410914012\n",
      "0.004352446939264025\n",
      "0.004352420049054282\n",
      "0.004352398446628026\n",
      "0.004352266103029251\n",
      "0.004352204906088965\n",
      "0.004352211854287556\n",
      "0.004352130992071969\n",
      "0.004352085892643247\n",
      "0.004352042602641242\n",
      "0.004351998439856938\n",
      "0.004351938396692276\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_index, (inputs,labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(inputs)[0]\n",
    "        loss = criterion(outputs,labels)  # 计算loss\n",
    "        loss.backward()     # loss 求导\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()  \n",
    "        \n",
    "    print(running_loss/7000)  \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6665484797099547"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_pred,yy_ture=torch.Tensor([[0,0,0]]),torch.tensor([0])\n",
    "for batch_index, (inputs,labels) in enumerate(train_loader):\n",
    "    inputs=inputs.to(device)\n",
    "    labels=labels.to(device)\n",
    "    outputs = model(inputs)[0].cpu()\n",
    "    labels=labels.cpu()\n",
    "    yy_ture=torch.cat((yy_ture,labels),axis=0)\n",
    "    yy_pred=torch.cat((yy_pred,outputs),axis=0)\n",
    "yy_pred=yy_pred.detach().numpy()[1:]\n",
    "yy_ture=yy_ture.detach().numpy()[1:]\n",
    "y_pred=np.argmax(yy_pred,axis=1)\n",
    "metrics.f1_score(yy_ture,y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3775268657701909"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_pred,yy_ture=torch.Tensor([[0,0,0]]),torch.tensor([0])\n",
    "for batch_index, (inputs,labels) in enumerate(validation_loader):\n",
    "    inputs=inputs.to(device)\n",
    "    labels=labels.to(device)\n",
    "    outputs = model(inputs)[0].cpu()\n",
    "    labels=labels.cpu()\n",
    "    yy_ture=torch.cat((yy_ture,labels),axis=0)\n",
    "    yy_pred=torch.cat((yy_pred,outputs),axis=0)\n",
    "yy_pred=yy_pred.detach().numpy()[1:]\n",
    "yy_ture=yy_ture.detach().numpy()[1:]\n",
    "y_pred=np.argmax(yy_pred,axis=1)\n",
    "metrics.f1_score(yy_ture,y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.7155\n",
      "1    0.1690\n",
      "2    0.1155\n",
      "Name: pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "outputs=model(x_test.cuda())\n",
    "pred=outputs[0].cpu().detach().numpy()\n",
    "pred = np.argmax(pred, axis=1)\n",
    "sub = test.groupby('ship').count().reset_index()[['ship']]\n",
    "sub['pred'] = pred\n",
    "\n",
    "print(sub['pred'].value_counts(1))\n",
    "sub['pred'] = sub['pred'].map(type_map_rev)\n",
    "sub.to_csv('128-cnn-result.csv', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(x_test.cuda())\n",
    "test_feature=pd.DataFrame(outputs[1][0].cpu().detach().numpy())\n",
    "test_feature.to_csv('test_cnn_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(x_train.cuda())\n",
    "train_feature=pd.DataFrame(outputs[1][0].cpu().detach().numpy())\n",
    "train_feature.to_csv('train_cnn_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model_cnn_state_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'model_cnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
